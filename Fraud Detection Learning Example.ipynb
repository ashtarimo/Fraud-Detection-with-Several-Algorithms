{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cufflinks as cf\nfrom scipy import stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import f_regression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve, GridSearchCV\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\ncf.go_offline()\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the dataset\ndf = pd.read_csv('../input/is-it-fraud/isfraud.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see how much data is missing, the percentage of missing data for each column is a good estimate, say if a column is missing a big percentage, say 80%, probably using it might not be a wise idea.\n100 * (df.isnull().sum()) / len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1- Defining and balancing the label","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['isFraud'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's label the target column, simply target\ndf['target'] = df['isFraud'].copy()\ndf = df.drop('isFraud', axis = 1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# is the label balanced? a dataset with balanced labels is ideal, where 0 and 1 values of target are almost equal in frequency\nsns.countplot(x = 'target', data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# balance the input for target, if only there are to values 0 and 1 as in val1 and val2\n\n# first shuffle indices:\ndf = df.sample(frac=1).reset_index(drop=True)\n\n# then set val1 and val2 below:\nval1 = 0\nval2 = 1\n\nval_1_bigger = 0\nval_2_bigger = 0\n\n# find the number of each label\nnum_val1 = df[df['target'] == val1].shape[0]\nnum_val2 = df[df['target'] == val2].shape[0]\n\nif num_val1 > num_val2:\n    val_1_bigger = 1\nelse:\n    val_2_bigger = 1\n\ni_1_count = 0\ni_2_count = 0\nindices_to_remove = []\n\nfor i in np.arange(len(df)):\n    \n    if df['target'][i] == 0:\n        i_1_count = i_1_count + 1\n        if (val_1_bigger == 1) and (i_1_count > num_val2):\n            indices_to_remove.append(i)\n    if df['target'][i] == 1:\n        i_2_count = i_2_count + 1\n        if (val_2_bigger == 1) and (i_2_count > num_val1):\n            indices_to_remove.append(i)\n\n            \ndf = df.drop(index=indices_to_remove, axis = 0)\ndf = df.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# is the label balanced, now?\nsns.countplot(x = 'target', data = df)\n\n# no but reall ywe cannot do that much about it","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2- droping clearly useless features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first the categorical ones\nfor col in df.select_dtypes(['object']):\n    print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# below codes compare the value counts in each categorical dataset to dataset length. The idea is that a dataset with a column with too many\n# distinct values is not useful for modeling.\n\ncol_list = []\nfactor = []\nnu = []\n\nfor col in df.select_dtypes(['object']):\n    col_list.append(col)\n    factor.append(100 * df[col].nunique() / len(df))\n    nu.append(df[col].nunique())\n\ncol_list = np.array(col_list).T\nfactor = np.array(factor).T\nnu = np.array(nu).T\n\nfactor_df = pd.DataFrame(data = col_list, columns = ['Column'])\nfactor_df['Factor'] = factor\nfactor_df['nu'] = nu\nfactor_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here, \"nameOrig\" and \"nameDest\" are not helpful in bulding any model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# so clearly nameOrig, and nameDest must be deleted\ndf = df.drop(['nameDest', 'nameOrig'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3- Which features are important for this target?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3-a) for number features, we examin corr and p_value. if corr < some% and value > 0.05 with the label, we will drop it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.abs(df.corr()['target']).sort_values(ascending = True)[:-1].plot.bar(figsize = (16,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets drop newbalanceDest and isFlaggedFraud, looks to be note that much linearly correlated","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now look at p_values\ncol_list = []\np_list = []\nfor col in df.select_dtypes(['number']):\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df[col], df['target'])\n    col_list.append(col)\n    p_list.append(p_value)\n    #print(f'{col} is associated with the target wtih p_value of:    {p_value}')\n\npval_table = pd.DataFrame(data = col_list, columns = ['col'])\npval_table['p_values'] = p_list\npval_table.sort_values(by = 'p_values', ascending = False)\n\n# same as corr results, so we need to be dropped two columns,","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['newbalanceDest', 'isFlaggedFraud'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3-b) Now let's look at target dependance on categorical","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.select_dtypes(['object']):\n    print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we try to distplot each column with hue of target\n\nfor col in df.select_dtypes(['object']):\n    plt.figure(figsize = (16,6))\n    sns.countplot(df[col], hue = df['target'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# based on the above, we can see that, type is correlated with our target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4- looking into each categorical column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets look into each object column\nfor col in df.select_dtypes(['object']):\n    print()\n    print('for the feature:     ', col)\n    print(df[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# no need to drop anything here, the distinct values are almost properly distributed, no outlier ... . Probably one can drop DEBIT values \n# as we dont have a lot of them. but for now, lets keep it","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5- Dealing with missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(100 * (df.isnull().sum()) / len(df)).sort_values(ascending = False)\n# here there is no missing value, but feel free to look at below cells for suh a senario.\n# if a column has many missing values, one might drop it.\n# one possible senario is to use fillna and use, say average of other values in the column\n# the other case, is to find what column is correlated with the column with missing value, and try to use this reference column to predict\n# what value we can kinda safely assign to the missing column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's assume \"amount\" column has missing values:\nnp.abs(df.corr()['amount']).sort_values()[:-1].plot.bar(figsize = (16, 8))\n# looks more correlations with OldBalanceOrg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,8))\nsns.distplot(df['oldbalanceOrg'].dropna(), bins = 50)\n# we use this graph to make bins for the next cell","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# the column with missing value is called \"my_missing\" and the reference column used to fill in the missing data is \"my_ref\"\n# here, based on the reference column value distirbution, we better define some bins. these bins of the reference are used to calculate\n# the mean for the missing column. then for a missing value, we look into the value of its associated reference column bins. There is one mean\n# value associated with each bin in the reference. Choosing bin needs to be smart and cannot be conveniently automated.\n# one should keep a balance between number of bins and length of dataset.\n# again, there is no missing value so we dont run this cell.\n\nmy_ref= 'oldbalanceOrg'\nmy_missing = 'amount'\n\nbins = [0, 0.1, 0.2, 0.3, 0.4, 0.6, 0.8, 1.0, 1.5, 2.5, 7.0]\n#my_min_val = bins[:-1]\n#my_max_val = bins[1:]\n\n#df_x = pd.DataFrame(df.dropna())\n#df_x = df_x.groupby(pd.cut(df_x[my_ref], bins))[my_missing].mean().to_frame()\n#df_x.reset_index(inplace = True)\n#df_x['my_min'] = my_min_val\n#df_x['my_max'] = my_max_val\n#df_x.drop(my_ref, axis = 1, inplace = True)\n\n#def fill_corr(ref, missing):\n#    if np.isnan(missing):\n#        for i in range(len(df_x)):\n#            if ref >= df_x.iloc[i,1] and ref < df_x.iloc[i,2]:\n#                return df_x.iloc[i, 0]\n#    else:\n#        return missing\n#\n#df[my_missing] = df.apply(lambda x: fill_corr(x[my_ref], x[my_missing]), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6- Getting rid of outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.select_dtypes(['number']):\n    print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.select_dtypes(['number']):\n    plt.figure(figsize = (16,6))\n    sns.distplot(df[col], bins = 100, kde = False)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# in this case, I dont any outliers to be a problem that much. one can go and try to cut it at some value and see the results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get rid of outliers. I use below code for doing so. I dont use quantile and prefer to look at data before ignoring it.\n# here we set up and down_temp values for the specified column\n\n#col = 'oldbalanceOrg'\n#up_temp = 20000000\n#down_temp = 0\n#plt.figure(figsize = (16,6))\n#sns.distplot(df[col], kde = False)\n#plt.show()\n#df = df[(df[col] < up_temp) & (df[col] > down_temp)]\n#print(df.shape[0])\n#plt.figure(figsize = (10,6))\n#sns.distplot(df[col], bins = 100)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7 look at it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# just looking into how each column is relating to the target column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.select_dtypes(['number']):\n    plt.figure(figsize = (16,6))\n    sns.scatterplot(data = df, x = col, y = 'target', hue = 'target')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.select_dtypes(['object']):\n    plt.figure(figsize = (16,6))\n    sns.countplot(df[col], hue = df['target'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8- get dummies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.select_dtypes(['object']):\n    print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.select_dtypes(['object']):\n    dummies = pd.get_dummies(df[col], drop_first = True, prefix = col)\n    df = pd.concat([df, dummies], axis = 1)\n    df = df.drop(col, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9- Train-Test split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting up x and y, the .values make it a numpy array to put into tf\n\nx = df.drop('target', axis = 1)\ny = df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split, first into train and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 101)\n\n# then split the train one into test and valid, 0.1111 x 0.9 = 0.09999\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.11111111, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape, x_val.shape, x_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10- Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## scaling must happen after test_train split to avoid data leakage and we dont \"fit\" the validation and test sets\n\nscalar = StandardScaler()\n\nx_train = scalar.fit_transform(x_train)\n\nx_val = scalar.transform(x_val)\n\nx_test = scalar.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 11- Modeling, we use different methods","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <center>SVC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we gotta use a grid search to find best parameters\nparam_grid = {'C' : [ 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000], 'gamma' : [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]}\ngrid = GridSearchCV(SVC(), param_grid, verbose = 2)\ngrid.fit(x_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we re-run it:\n# predictions\n\npredictions = pd.DataFrame(grid.predict(x_test), columns = ['Predicted Values'])\npredictions['Real Values'] = y_test.reset_index(drop = True)\npredictions.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# residual making\n\npredictions['Residual'] = predictions['Real Values'] - predictions['Predicted Values']\nsns.countplot(predictions['Residual'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_test, predictions['Predicted Values']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"my_cm = confusion_matrix(y_test, predictions['Predicted Values'])\nconf_temp = {'Predicted NO': [my_cm[0][0], my_cm[1][0]], 'Predicted YES': [my_cm[0][1], my_cm[1][1]] }\nmy_cmdf = pd.DataFrame(conf_temp, index = ['Actual NO', 'Actual YES'])\nmy_cmdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#d = classification_report(y_test, predictions['Predicted Values'], output_dict=True)\n#correct_percentage_svc = d['accuracy'] * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"correct_percentage_svc = 100 * (my_cm[0][0] + my_cm[1][1]) / predictions.shape[0]\nwrong_percentage = 100 - correct_percentage_svc\nprint(f'Model Accuracy is: { correct_percentage_svc:.4}%' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <center> Logistic","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# modeling and priting off coefs:\n\nlm = LogisticRegression(max_iter = 1000)\nlm.fit(x_train, y_train.ravel())\n\n# setting coeffs\nmy_coef = lm.coef_\n\n# setting intercept\nmy_intercept = lm.intercept_\n\nprint(my_coef)\nprint('')\nprint(my_intercept)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions\n\npredictions = pd.DataFrame(lm.predict(x_test), columns = ['Predicted Values'])\npredictions ['Real Values'] = y_test.reset_index(drop = True)\npredictions.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making a data frame for real vs predicted vs residuals\npredictions ['Residuals'] = predictions ['Real Values'] - predictions['Predicted Values']\nsns.countplot(predictions['Residuals'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, predictions['Predicted Values']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_cm = confusion_matrix(y_test, predictions['Predicted Values'])\nconf_temp = {'Predicted NO': [my_cm[0][0], my_cm[1][0]], 'Predicted YES': [my_cm[0][1], my_cm[1][1]] }\nmy_cmdf = pd.DataFrame(conf_temp, index = ['Actual NO', 'Actual YES'])\nmy_cmdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct_percentage_log = 100 * (my_cm[0][0] + my_cm[1][1]) / predictions.shape[0]\nwrong_percentage = 100 - correct_percentage_log\nprint(f'Model Accuracy is: {correct_percentage_log:.4}%' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <center> Dicision Trees","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree = DecisionTreeClassifier()\ndtree.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions\n\npredictions = pd.DataFrame(dtree.predict(x_test), columns = ['Predicted Values'])\npredictions ['Real Values'] = y_test.reset_index(drop = True)\npredictions.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# residual making\n\npredictions['Residual'] = predictions['Real Values'] - predictions['Predicted Values']\nsns.countplot(predictions['Residual'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, predictions['Predicted Values']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_cm = confusion_matrix(y_test, predictions['Predicted Values'])\nconf_temp = {'Predicted NO': [my_cm[0][0], my_cm[1][0]], 'Predicted YES': [my_cm[0][1], my_cm[1][1]] }\nmy_cmdf = pd.DataFrame(conf_temp, index = ['Actual NO', 'Actual YES'])\nmy_cmdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct_percentage_dt = 100 * (my_cm[0][0] + my_cm[1][1]) / predictions.shape[0]\nwrong_percentage = 100 - correct_percentage_dt\nprint(f'Model Accuracy is: {correct_percentage_dt:.4}%' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <center> Random Forests","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 1000)\nrf.fit(x_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions\n\npredictionsrf = pd.DataFrame(rf.predict(x_test), columns = ['Predicted Values'])\npredictionsrf ['Real Values'] = y_test.reset_index(drop = True)\npredictionsrf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# residual making\n\npredictions['Residual'] = predictions['Real Values'] - predictions['Predicted Values']\nsns.countplot(predictions['Residual'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, predictionsrf['Predicted Values']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_cm = confusion_matrix(y_test, predictionsrf['Predicted Values'])\nconf_temp = {'Predicted NO': [my_cm[0][0], my_cm[1][0]], 'Predicted YES': [my_cm[0][1], my_cm[1][1]] }\nmy_cmdf = pd.DataFrame(conf_temp, index = ['Actual NO', 'Actual YES'])\nmy_cmdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct_percentage_rf = 100 * (my_cm[0][0] + my_cm[1][1]) / predictionsrf.shape[0]\nwrong_percentage = 100 - correct_percentage_rf\nprint(f'Model Accuracy is: {correct_percentage_rf:.4}%' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <center> ANN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.drop('target', axis = 1).values\ny = df['target'].values\n\n# split, first into train and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 101)\n\n# then split the train one into test and valid\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.11111111, random_state = 101)\n\n## scaling must happen after test_train split to avoid data leakage\n\nscalar = StandardScaler()\n\nx_train = scalar.fit_transform(x_train)\n\nx_val = scalar.transform(x_val)\n\nx_test = scalar.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making the layers\n\nmodel = Sequential()\nnnodes = 250\nact_func = 'relu'\n\nmodel.add(Dense(nnodes, activation = act_func))\n\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\nmodel.add(Dense(nnodes, activation = act_func))\n\n\nmodel.add(Dense(1, activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compiling the model\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitting the model\n\nearly_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 25, verbose = 1)\n\nmodel.fit(x = x_train, y = y_train, validation_data = (x_val, y_val),\n          callbacks = [early_stop],\n          batch_size = 256,\n          epochs = 500, \n          verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_func = pd.DataFrame(data = model.history.history['loss'], columns = ['Model Loss'])\nloss_func['Validation Loss'] =  model.history.history['val_loss']\nloss_func","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_func.plot(figsize = (15,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate against a test set\nprint(model.evaluate(x_test, y_test, verbose = 0))\nprint(model.evaluate(x_train, y_train, verbose = 0))\nprint(model.evaluate(x_val, y_val, verbose = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pd.DataFrame(data = model.predict_classes(x_test), columns = ['Predicted Values'])\npredictions['Real Values'] = y_test\npredictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# residual making\n\npredictions['Residual'] = predictions['Real Values'] - predictions['Predicted Values']\nsns.countplot(predictions['Residual'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy predictor\nprint(classification_report(predictions['Real Values'], predictions['Predicted Values']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confustion matrix\nClass_0 = '0'\nClass_1 = '1'\n\nmy_cm = confusion_matrix(y_test, predictions['Predicted Values'])\nconf_temp = {f'Predicted {Class_0}': [my_cm[0][0], my_cm[1][0]], f'Predicted {Class_1}': [my_cm[0][1], my_cm[1][1]] }\nmy_cmdf = pd.DataFrame(conf_temp, index = [f'Actual {Class_0}', f'Actual {Class_1}'])\nmy_cmdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct_percentage_ann = 100 * (my_cm[0][0] + my_cm[1][1]) / predictions.shape[0]\nwrong_percentage_ann = 100 - correct_percentage_ann\nprint(f'Model Accuracy is: {correct_percentage_ann:.4}%' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 12- Models' Comparison","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"ind = ['SVC', 'Logistics', 'Dicision Tree', 'Random Forests', 'ANN']\nvalues = [correct_percentage_svc, correct_percentage_log, correct_percentage_dt, correct_percentage_rf, correct_percentage_ann]\n\ndf = pd.DataFrame(data = values, index = ind, columns = ['%Accuracy']).sort_values(by = '%Accuracy', ascending = False).round(2)\ndf","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}